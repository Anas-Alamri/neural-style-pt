{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG-19 Architecture Detected\n",
      "Successfully loaded models/vgg19-d01eb7cb.pth\n",
      "conv1_1: 64 3 3 3\n",
      "conv1_2: 64 64 3 3\n",
      "conv2_1: 128 64 3 3\n",
      "conv2_2: 128 128 3 3\n",
      "conv3_1: 256 128 3 3\n",
      "conv3_2: 256 256 3 3\n",
      "conv3_3: 256 256 3 3\n",
      "conv3_4: 256 256 3 3\n",
      "conv4_1: 512 256 3 3\n",
      "conv4_2: 512 512 3 3\n",
      "conv4_3: 512 512 3 3\n",
      "conv4_4: 512 512 3 3\n",
      "conv5_1: 512 512 3 3\n",
      "conv5_2: 512 512 3 3\n",
      "conv5_3: 512 512 3 3\n",
      "conv5_4: 512 512 3 3\n",
      "Setting up style layer 2: relu1_1\n",
      "Setting up style layer 7: relu2_1\n",
      "Setting up style layer 12: relu3_1\n",
      "Setting up style layer 21: relu4_1\n",
      "Setting up content layer 23: relu4_2\n",
      "Setting up style layer 30: relu5_1\n",
      "Sequential(\n",
      "  (0): TVLoss()\n",
      "  (1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (2): ReLU(inplace=True)\n",
      "  (3): MaskedStyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (crit): MSELoss()\n",
      "  )\n",
      "  (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (5): ReLU(inplace=True)\n",
      "  (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (8): ReLU(inplace=True)\n",
      "  (9): MaskedStyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (crit): MSELoss()\n",
      "  )\n",
      "  (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (11): ReLU(inplace=True)\n",
      "  (12): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (13): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (14): ReLU(inplace=True)\n",
      "  (15): MaskedStyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (crit): MSELoss()\n",
      "  )\n",
      "  (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (17): ReLU(inplace=True)\n",
      "  (18): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (19): ReLU(inplace=True)\n",
      "  (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (21): ReLU(inplace=True)\n",
      "  (22): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (23): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (24): ReLU(inplace=True)\n",
      "  (25): MaskedStyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (crit): MSELoss()\n",
      "  )\n",
      "  (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (27): ReLU(inplace=True)\n",
      "  (28): ContentLoss(\n",
      "    (crit): MSELoss()\n",
      "  )\n",
      "  (29): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (30): ReLU(inplace=True)\n",
      "  (31): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (32): ReLU(inplace=True)\n",
      "  (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (35): ReLU(inplace=True)\n",
      "  (36): MaskedStyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (crit): MSELoss()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def optimize(stylenet, img, num_iterations, output_path, original_colors, print_iter=None, save_iter=None):\n",
    "    def iterate():\n",
    "        t[0] += 1\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        stylenet(img)\n",
    "        loss = stylenet.get_loss()\n",
    "        loss.backward()\n",
    "        \n",
    "        maybe_print(stylenet, t[0], print_iter, num_iterations, loss)\n",
    "        maybe_save(img, t[0], save_iter, num_iterations, original_colors, output_path)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    img = nn.Parameter(img.type(stylenet.dtype))\n",
    "    optimizer, loopVal = setup_optimizer(img, stylenet.params, num_iterations)\n",
    "    t = [0]\n",
    "    while t[0] <= loopVal:\n",
    "        optimizer.step(iterate)\n",
    "    \n",
    "    return img\n",
    "    \n",
    "from model import *\n",
    "from utils import *\n",
    "\n",
    "# setup stylenet\n",
    "params = StylenetArgs()\n",
    "params.gpu = '0'\n",
    "params.backend = 'cudnn'\n",
    "\n",
    "dtype, multidevice, backward_device = setup_gpu(params)\n",
    "stylenet = StyleNet(params, dtype, multidevice, backward_device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 512\n",
    "num_iterations = 500\n",
    "output_path = 'out.png'\n",
    "\n",
    "original_colors = 0\n",
    "style_scale = 1.0\n",
    "print_iter = 100\n",
    "save_iter = 100\n",
    "\n",
    "content_path = 'examples/inputs/monalisa.jpg'\n",
    "style_paths = ['examples/inputs/starry_night.jpg','examples/inputs/cubist.jpg']\n",
    "\n",
    "# load content image\n",
    "content_image = load_image(content_path, image_size)\n",
    "\n",
    "# load style images\n",
    "style_size = int(image_size * style_scale)\n",
    "style_images = [load_image(path, style_size) for path in style_paths]\n",
    "\n",
    "# set hyper-parameters\n",
    "stylenet.set_content_weight(5e0)\n",
    "stylenet.set_style_weight(1e2)\n",
    "stylenet.set_tv_weight(1e-3)\n",
    "\n",
    "# capture the style and content images\n",
    "stylenet.capture(content_image, style_images)\n",
    "\n",
    "# initialize with a random image\n",
    "img = random_image_like(content_image)\n",
    "\n",
    "optimize(stylenet, img, num_iterations, output_path, original_colors, print_iter, save_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "image_size = 512\n",
    "num_iterations = 500\n",
    "output_path = 'out2.png'\n",
    "original_colors = 0\n",
    "style_scale = 1.0\n",
    "print_iter = 100\n",
    "save_iter = 100\n",
    "\n",
    "\n",
    "content_path = 'examples/inputs/monalisa.jpg'\n",
    "style_paths = ['examples/inputs/starry_night.jpg','examples/inputs/cubist.jpg']\n",
    "content_mask_paths = ['examples/segments/monalisa1a.png','examples/segments/monalisa1b.png']\n",
    "\n",
    "# load content image\n",
    "content_image = load_image(content_path, image_size)#.type(dtype)\n",
    "\n",
    "# load style images\n",
    "style_size = int(image_size * style_scale)\n",
    "style_images = [load_image(path, style_size) \n",
    "                for path in style_paths]\n",
    "\n",
    "# load masks\n",
    "content_masks = [load_image(path, image_size, to_normalize=False) \n",
    "                 for path in content_mask_paths]\n",
    "\n",
    "# capture the style and content images (with masks)\n",
    "stylenet.capture(content_image, style_images, None, content_masks)\n",
    "            \n",
    "# set hyper-parameters\n",
    "stylenet.set_content_weight(5e0)\n",
    "stylenet.set_style_weight(1e2)\n",
    "stylenet.set_tv_weight(1e-3)\n",
    "\n",
    "# initialize with a random image\n",
    "img = random_image_like(content_image)\n",
    "\n",
    "optimize(stylenet, img, num_iterations, output_path, original_colors, print_iter, save_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1000/1000: \n",
      "  Content loss = 0.0e+00\n",
      "  Style loss = 2.4e+04, 3.1e+06, 1.2e+06, 4.0e+07, 3.0e+03\n",
      "  TV loss = 3.5e+04\n",
      "  Total loss = 4.39e+07\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[ 107.4170,  107.3822,  107.2753,  ...,   18.5964,   18.6580,\n",
       "             18.6587],\n",
       "          [ 107.4086,  107.3838,  107.2558,  ...,   18.5873,   18.6362,\n",
       "             18.6319],\n",
       "          [   6.0468,    6.0569,   26.0255,  ...,   18.5962,   18.6259,\n",
       "             18.6241],\n",
       "          ...,\n",
       "          [ -15.5395,  -15.5464,  -15.5937,  ...,   -4.7783,   -4.8542,\n",
       "             -4.8807],\n",
       "          [ -15.6088,  -15.5838,  -15.5704,  ...,   -4.8293,   -4.8587,\n",
       "             -4.8855],\n",
       "          [ -15.5850,  -15.5833,  -15.5725,  ...,   -4.8222,   -4.8541,\n",
       "             -4.8611]],\n",
       "\n",
       "         [[  20.6844,   20.6578,   20.5571,  ...,   20.3936,   20.4353,\n",
       "             20.4430],\n",
       "          [  20.6818,   20.6415,   20.5252,  ...,   20.3572,   20.4206,\n",
       "             20.4426],\n",
       "          [ -93.5266,  -94.0915,  -97.6999,  ...,   20.3577,   20.4144,\n",
       "             20.4342],\n",
       "          ...,\n",
       "          [ -40.6276,  -40.6149,  -40.6026,  ...,   13.2112,   13.2194,\n",
       "             13.2305],\n",
       "          [ -40.5604,  -40.5715,  -40.5721,  ...,   13.2204,   13.2089,\n",
       "             13.2494],\n",
       "          [ -40.5617,  -40.5551,  -40.5580,  ...,   13.2325,   13.2305,\n",
       "             13.1608]],\n",
       "\n",
       "         [[ -71.0882,  -71.1080,  -71.9495,  ...,   23.5993,   23.8394,\n",
       "             23.8973],\n",
       "          [ -71.1370,  -71.1350,  -71.9137,  ...,   23.5728,   23.8488,\n",
       "             23.8602],\n",
       "          [-116.0931, -113.4171, -113.0039,  ...,   23.5603,   23.8784,\n",
       "             23.9017],\n",
       "          ...,\n",
       "          [ -35.7993,  -35.8265,  -35.8149,  ...,   26.8732,   26.8550,\n",
       "             26.8414],\n",
       "          [ -35.7822,  -35.8037,  -35.8361,  ...,   26.8806,   26.8423,\n",
       "             26.8359],\n",
       "          [ -35.7844,  -35.7783,  -35.8108,  ...,   26.8794,   26.8500,\n",
       "             26.8926]]]], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#######################################################3\n",
    "\n",
    "\n",
    "image_size = 1280\n",
    "num_iterations = 1200\n",
    "output_path = 'go5.png'\n",
    "\n",
    "original_colors = 0\n",
    "style_scale = 0.25\n",
    "print_iter = 100\n",
    "save_iter = 100\n",
    "\n",
    "content_path = 'examples/inputs/hoovertowernight.jpg'\n",
    "\n",
    "\n",
    "style_paths = ['examples/inputs/starry_night.jpg','examples/inputs/hokusai.jpg',\n",
    "                'examples/inputs/starry_night.jpg','examples/inputs/hokusai.jpg',\n",
    "                'examples/inputs/starry_night.jpg','examples/inputs/hokusai.jpg',\n",
    "                'examples/inputs/starry_night.jpg','examples/inputs/hokusai.jpg',\n",
    "                'examples/inputs/starry_night.jpg','examples/inputs/hokusai.jpg',\n",
    "                'examples/inputs/starry_night.jpg','examples/inputs/hokusai.jpg']\n",
    "\n",
    "\n",
    "style_blend_weights = [1.0, 0.0,\n",
    "                      0.8, 0.2,\n",
    "                       0.6, 0.4,\n",
    "                       0.4, 0.6,\n",
    "                       0.2, 0.8,                      \n",
    "                      0.0, 1.0]\n",
    "\n",
    "# load content image\n",
    "content_image = load_image(content_path, image_size)\n",
    "\n",
    "# load style images\n",
    "style_size = int(image_size * style_scale)\n",
    "style_images = [load_image(path, style_size) for path in style_paths]\n",
    "\n",
    "stylenet.set_content_weight(0)\n",
    "stylenet.set_style_weight(1e2)\n",
    "stylenet.set_tv_weight(1e-3)\n",
    "\n",
    "# capture the style and content images\n",
    "\n",
    "# initialize with a random image\n",
    "img = random_image_like(content_image)\n",
    "\n",
    "\n",
    "\n",
    "content_masks = [torch.zeros(content_image.shape) for c in range(len(style_images))]\n",
    "m=8\n",
    "content_masks[0][:,:,:,:360+m] = 1.0\n",
    "content_masks[1][:,:,:,:360+m] = 1.0\n",
    "content_masks[2][:,:,:,360-m:500+m] = 1.0\n",
    "content_masks[3][:,:,:,360-m:500+m] = 1.0\n",
    "content_masks[4][:,:,:,500-m:640+m] = 1.0\n",
    "content_masks[5][:,:,:,500-m:640+m] = 1.0\n",
    "content_masks[6][:,:,:,640-m:780+m] = 1.0\n",
    "content_masks[7][:,:,:,640-m:780+m] = 1.0\n",
    "content_masks[8][:,:,:,780-m:920+m] = 1.0\n",
    "content_masks[9][:,:,:,780-m:920+m] = 1.0\n",
    "content_masks[10][:,:,:,920-m:] = 1.0\n",
    "content_masks[11][:,:,:,920-m:] = 1.0\n",
    "\n",
    "\n",
    "stylenet.capture(content_image, style_images, style_blend_weights, content_masks)\n",
    "\n",
    "optimize(stylenet, img, 1000, output_path, original_colors, print_iter, save_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (1280) must match the existing size (720) at non-singleton dimension 3.  Target sizes: [1, 3, 673, 1280].  Tensor sizes: [720]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-40ac66a054c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mcontent_masks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstyle_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mcontent_masks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mcontent_masks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m720\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mcontent_masks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mcontent_masks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m720\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (1280) must match the existing size (720) at non-singleton dimension 3.  Target sizes: [1, 3, 673, 1280].  Tensor sizes: [720]"
     ]
    }
   ],
   "source": [
    "import IPython.display\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "from io import BytesIO \n",
    "import IPython.display\n",
    "import numpy as np\n",
    "\n",
    "def showarray(a, fmt='png'):\n",
    "    a = np.uint8(np.array(a))\n",
    "    f = BytesIO()\n",
    "    PIL.Image.fromarray(a).save(f, fmt)\n",
    "    IPython.display.display(IPython.display.Image(data=f.getvalue()))\n",
    "\n",
    "x = 0\n",
    "content_masks = [torch.zeros(content_image.shape) for c in range(len(style_images))]\n",
    "content_masks[0][:,:,:,:x] = torch.linspace(0, 1, x)\n",
    "content_masks[0][:,:,:,x:] = torch.linspace(1, 0, 720-x)\n",
    "content_masks[1][:,:,:,:x] = torch.linspace(0, 1, x)\n",
    "content_masks[1][:,:,:,x:] = torch.linspace(1, 0, 720-x)\n",
    "\n",
    "showarray(np.transpose(np.array(255*content_masks[0][0]), [1,2,0]).astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#######################################################3\n",
    "\n",
    "\n",
    "image_size = 720\n",
    "num_iterations = 1200\n",
    "output_path = 'go6.png'\n",
    "\n",
    "original_colors = 0\n",
    "style_scale = 1.0\n",
    "print_iter = 100\n",
    "save_iter = 100\n",
    "\n",
    "content_path = 'examples/inputs/hoovertowernight.jpg'\n",
    "style_paths = ['examples/inputs/starry_night.jpg','examples/inputs/hokusai.jpg']\n",
    "\n",
    "# load content image\n",
    "content_image = load_image(content_path, image_size)\n",
    "\n",
    "# load style images\n",
    "style_size = int(image_size * style_scale)\n",
    "style_images = [load_image(path, style_size) for path in style_paths]\n",
    "\n",
    "stylenet.set_content_weight(0)\n",
    "stylenet.set_style_weight(1e2)\n",
    "stylenet.set_tv_weight(1e-3)\n",
    "\n",
    "# capture the style and content images\n",
    "\n",
    "# initialize with a random image\n",
    "img = random_image_like(content_image)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "content_masks = [torch.zeros(content_image.shape) for c in range(len(style_images))]\n",
    "content_masks[0][:,:,:,:] = torch.linspace(0, 1, 720)\n",
    "content_masks[1][:,:,:,:] = torch.linspace(1, 0, 720)\n",
    "#content_masks[2][:,:,:,480:] = 1.0\n",
    "\n",
    "def optimize2(stylenet, img, num_iterations, output_path, original_colors, print_iter=None, save_iter=None):\n",
    "    def iterate():\n",
    "        t[0] += 1\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        stylenet(img)\n",
    "        loss = stylenet.get_loss()\n",
    "        loss.backward()\n",
    "        \n",
    "        maybe_print(stylenet, t[0], print_iter, num_iterations, loss)\n",
    "        maybe_save(img, t[0], save_iter, num_iterations, original_colors, output_path)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    img = nn.Parameter(img.type(stylenet.dtype))\n",
    "    optimizer, loopVal = setup_optimizer(img, stylenet.params, num_iterations)\n",
    "    t = [0]\n",
    "    while t[0] <= loopVal:\n",
    "        optimizer.step(iterate)\n",
    "    \n",
    "    return img\n",
    "\n",
    "\n",
    "\n",
    "for z in range(8):\n",
    "    for x in range(0,720,3):\n",
    "        if z%2==0:\n",
    "            content_masks = [torch.zeros(content_image.shape) for c in range(len(style_images))]\n",
    "            content_masks[0][:,:,:,:x] = torch.linspace(0, 1, x)\n",
    "            content_masks[0][:,:,:,x:] = torch.linspace(1, 0, 720-x)\n",
    "            content_masks[1][:,:,:,:x] = torch.linspace(0, 1, x)\n",
    "            content_masks[1][:,:,:,x:] = torch.linspace(1, 0, 720-x)\n",
    "            bw = x/719.0;\n",
    "            style_blend_weights = [bw, 1.0-bw]\n",
    "        else:\n",
    "            content_masks = [torch.zeros(content_image.shape) for c in range(len(style_images))]\n",
    "            content_masks[0][:,:,:,:720-x] = torch.linspace(0, 1, x)\n",
    "            content_masks[0][:,:,:,720-x:] = torch.linspace(1, 0, 720-x)\n",
    "            content_masks[1][:,:,:,:720-x] = torch.linspace(0, 1, x)\n",
    "            content_masks[1][:,:,:,720-x:] = torch.linspace(1, 0, 720-x)\n",
    "            bw = (719.0-x)/719.0;\n",
    "            style_blend_weights = [bw, 1.0-bw]\n",
    "\n",
    "        stylenet.capture(content_image, style_images, None, content_masks)\n",
    "\n",
    "        img = optimize2(stylenet, img, 5, output_path, original_colors, print_iter, save_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def optimize(stylenet, img, num_iterations, output_path, original_colors, print_iter=None, save_iter=None):\n",
    "    def iterate():\n",
    "        t[0] += 1\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        stylenet(img)\n",
    "        loss = stylenet.get_loss()\n",
    "        loss.backward()\n",
    "        \n",
    "        maybe_print(stylenet, t[0], print_iter, num_iterations, loss)\n",
    "        maybe_save(img, t[0], save_iter, num_iterations, original_colors, output_path)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    img = nn.Parameter(img.type(stylenet.dtype))\n",
    "    optimizer, loopVal = setup_optimizer(img, stylenet.params, num_iterations)\n",
    "    t = [0]\n",
    "    while t[0] <= loopVal:\n",
    "        optimizer.step(iterate)\n",
    "    \n",
    "\n",
    "        \n",
    "image_size = 720\n",
    "num_iterations = 1000\n",
    "output_path = 'try9d.png'\n",
    "original_colors = 0\n",
    "style_scale = 1.0\n",
    "print_iter = 100\n",
    "save_iter = 100\n",
    "\n",
    "\n",
    "content_path = 'examples/inputs/hoovertowernight.jpg'\n",
    "style_paths = ['examples/inputs/starry_night.jpg', 'examples/inputs/hokusai1.jpg']\n",
    "\n",
    "# load content image\n",
    "content_image = load_image(content_path, image_size)#.type(dtype)\n",
    "\n",
    "# load style images\n",
    "style_size = int(image_size * style_scale)\n",
    "style_images = [load_image(path, style_size) \n",
    "                for path in style_paths]\n",
    "\n",
    "#style_images[1] = torch.cat([style_images[0], style_images[2]], axis=2)\n",
    "\n",
    "# load masks\n",
    "content_masks = [torch.zeros(content_image.shape) for c in range(len(style_images))]\n",
    "content_masks[0][:,:,:,:] = torch.linspace(0, 1, 720)\n",
    "content_masks[1][:,:,:,:] = torch.linspace(1, 0, 720)\n",
    "#content_masks[2][:,:,:,480:] = 1.0\n",
    "\n",
    "# capture the style and content images (with masks)\n",
    "stylenet.capture(content_image, style_images, None, content_masks)\n",
    "            \n",
    "# set hyper-parameters\n",
    "stylenet.set_content_weight(0)\n",
    "stylenet.set_style_weight(1e2)\n",
    "stylenet.set_tv_weight(1e-3)\n",
    "\n",
    "# initialize with a random image\n",
    "img = random_image_like(content_image)\n",
    "\n",
    "optimize(stylenet, img, num_iterations, output_path, original_colors, print_iter, save_iter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
