{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG-19 Architecture Detected\n",
      "Successfully loaded models/vgg19-d01eb7cb.pth\n",
      "conv1_1: 64 3 3 3\n",
      "conv1_2: 64 64 3 3\n",
      "conv2_1: 128 64 3 3\n",
      "conv2_2: 128 128 3 3\n",
      "conv3_1: 256 128 3 3\n",
      "conv3_2: 256 256 3 3\n",
      "conv3_3: 256 256 3 3\n",
      "conv3_4: 256 256 3 3\n",
      "conv4_1: 512 256 3 3\n",
      "conv4_2: 512 512 3 3\n",
      "conv4_3: 512 512 3 3\n",
      "conv4_4: 512 512 3 3\n",
      "conv5_1: 512 512 3 3\n",
      "conv5_2: 512 512 3 3\n",
      "conv5_3: 512 512 3 3\n",
      "conv5_4: 512 512 3 3\n",
      "Setting up style layer 2: relu1_1\n",
      "Setting up style layer 7: relu2_1\n",
      "Setting up histogram layer 7: relu2_1\n",
      "Setting up style layer 12: relu3_1\n",
      "Setting up histogram layer 12: relu3_1\n",
      "Setting up style layer 21: relu4_1\n",
      "Setting up histogram layer 21: relu4_1\n",
      "Setting up content layer 23: relu4_2\n",
      "Setting up style layer 30: relu5_1\n",
      "Setting up histogram layer 30: relu5_1\n",
      "Sequential(\n",
      "  (0): TVLoss()\n",
      "  (1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (2): ReLU(inplace=True)\n",
      "  (3): MaskedStyleLoss(\n",
      "    (crit): MSELoss()\n",
      "    (gram): GramMatrix()\n",
      "  )\n",
      "  (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (5): ReLU(inplace=True)\n",
      "  (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (8): ReLU(inplace=True)\n",
      "  (9): MaskedStyleLoss(\n",
      "    (crit): MSELoss()\n",
      "    (gram): GramMatrix()\n",
      "  )\n",
      "  (10): MaskedHistLoss(\n",
      "    (crit): MSELoss()\n",
      "  )\n",
      "  (11): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (12): ReLU(inplace=True)\n",
      "  (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (15): ReLU(inplace=True)\n",
      "  (16): MaskedStyleLoss(\n",
      "    (crit): MSELoss()\n",
      "    (gram): GramMatrix()\n",
      "  )\n",
      "  (17): MaskedHistLoss(\n",
      "    (crit): MSELoss()\n",
      "  )\n",
      "  (18): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (19): ReLU(inplace=True)\n",
      "  (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (21): ReLU(inplace=True)\n",
      "  (22): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (23): ReLU(inplace=True)\n",
      "  (24): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (25): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (26): ReLU(inplace=True)\n",
      "  (27): MaskedStyleLoss(\n",
      "    (crit): MSELoss()\n",
      "    (gram): GramMatrix()\n",
      "  )\n",
      "  (28): MaskedHistLoss(\n",
      "    (crit): MSELoss()\n",
      "  )\n",
      "  (29): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (30): ReLU(inplace=True)\n",
      "  (31): ContentLoss(\n",
      "    (crit): MSELoss()\n",
      "  )\n",
      "  (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (33): ReLU(inplace=True)\n",
      "  (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (35): ReLU(inplace=True)\n",
      "  (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (38): ReLU(inplace=True)\n",
      "  (39): MaskedStyleLoss(\n",
      "    (crit): MSELoss()\n",
      "    (gram): GramMatrix()\n",
      "  )\n",
      "  (40): MaskedHistLoss(\n",
      "    (crit): MSELoss()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def optimize(stylenet, img, num_iterations, output_path, original_colors, print_iter=None, save_iter=None):\n",
    "    def iterate():\n",
    "        t[0] += 1\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        stylenet(img)\n",
    "        loss = stylenet.get_loss()\n",
    "        loss.backward()\n",
    "        \n",
    "        maybe_print(stylenet, t[0], print_iter, num_iterations, loss)\n",
    "        maybe_save(img, t[0], save_iter, num_iterations, original_colors, output_path)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    img = nn.Parameter(img.type(stylenet.dtype))\n",
    "    optimizer, loopVal = setup_optimizer(img, stylenet.params, num_iterations)\n",
    "    t = [0]\n",
    "    while t[0] <= loopVal:\n",
    "        optimizer.step(iterate)\n",
    "    \n",
    "    return img\n",
    "    \n",
    "from model import *\n",
    "from utils import *\n",
    "\n",
    "# setup stylenet\n",
    "params = StylenetArgs()\n",
    "params.gpu = '0'\n",
    "params.backend = 'cudnn'\n",
    "\n",
    "dtype, multidevice, backward_device = setup_gpu(params)\n",
    "stylenet = StyleNet(params, dtype, multidevice, backward_device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 500/500: \n",
      "  Content loss = 1.1e+06\n",
      "  Style loss = 9.3e+04, 3.4e+06, 2.2e+06, 1.0e+08, 1.8e+04\n",
      "  TV loss = 1.1e+04\n",
      "  Total loss = 1.07e+08\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[ 1.6174e+01,  2.1853e+01,  1.8234e+01,  ...,  4.1432e-01,\n",
       "            2.0268e-01,  3.5919e-01],\n",
       "          [ 2.6166e+01,  4.4644e+01,  3.3452e+01,  ...,  2.0497e+00,\n",
       "            1.2524e+00,  7.3811e-01],\n",
       "          [ 2.2696e+01,  4.0139e+01,  2.3072e+01,  ...,  2.3718e+00,\n",
       "            1.1570e+00,  8.3858e-01],\n",
       "          ...,\n",
       "          [ 7.0310e+00,  1.4105e+01,  1.0797e+01,  ...,  7.6945e+00,\n",
       "            7.8103e+00,  2.2036e+00],\n",
       "          [ 7.3547e+00,  4.4244e+00,  4.5356e+00,  ...,  9.6515e+00,\n",
       "            7.5546e+00,  3.9019e+00],\n",
       "          [ 7.4261e+00,  4.9958e+00,  2.1514e+00,  ...,  1.0535e+01,\n",
       "            8.0168e+00,  5.6048e+00]],\n",
       "\n",
       "         [[-6.9994e+00, -8.4963e+00, -6.4352e+00,  ..., -2.5566e-01,\n",
       "           -6.8582e-01, -3.8163e-01],\n",
       "          [-7.6155e+00, -5.6675e+00, -7.0233e+00,  ...,  3.9746e-01,\n",
       "           -5.8201e-01,  2.1621e-01],\n",
       "          [-1.3962e+01, -1.0485e+01, -1.2550e+01,  ..., -6.3458e-01,\n",
       "           -1.4765e+00,  2.8541e-02],\n",
       "          ...,\n",
       "          [-6.9234e+00,  1.3262e+00,  3.0114e+00,  ...,  1.4821e+00,\n",
       "            4.2723e+00,  1.3484e+00],\n",
       "          [-5.1174e+00, -8.8483e+00,  4.3617e+00,  ...,  1.6189e+00,\n",
       "            3.7674e+00,  3.8384e+00],\n",
       "          [ 2.3875e+00, -2.7363e+00,  1.9230e-01,  ...,  7.1429e+00,\n",
       "            6.7055e+00,  5.8079e+00]],\n",
       "\n",
       "         [[-2.4195e+01, -3.4365e+01, -3.0175e+01,  ..., -4.5063e+00,\n",
       "           -3.3869e+00, -1.5681e+00],\n",
       "          [-3.4112e+01, -4.2840e+01, -3.9448e+01,  ..., -5.0428e+00,\n",
       "           -3.1234e+00, -1.0186e+00],\n",
       "          [-3.6450e+01, -4.1841e+01, -4.0520e+01,  ..., -5.4855e+00,\n",
       "           -3.5325e+00, -1.0333e+00],\n",
       "          ...,\n",
       "          [-2.5482e+00,  4.7370e+00, -1.3131e+00,  ..., -9.5375e-01,\n",
       "           -2.4287e+00, -5.2772e+00],\n",
       "          [ 3.6004e+00, -1.5861e-01, -2.8976e+00,  ..., -4.4737e+00,\n",
       "           -2.7538e+00, -3.0643e+00],\n",
       "          [ 9.1246e+00,  4.4843e+00, -1.6417e+00,  ..., -8.4280e-01,\n",
       "           -3.5537e-03,  9.4058e-02]]]], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_size = 512\n",
    "num_iterations = 500\n",
    "output_path = 'out6.png'\n",
    "\n",
    "original_colors = 0\n",
    "style_scale = 1.0\n",
    "print_iter = 100\n",
    "save_iter = 100\n",
    "\n",
    "content_path = 'examples/inputs/monalisa.jpg'\n",
    "style_paths = ['examples/inputs/starry_night.jpg','examples/inputs/cubist.jpg']\n",
    "\n",
    "# load content image\n",
    "content_image = load_image(content_path, image_size)\n",
    "\n",
    "# load style images\n",
    "style_size = int(image_size * style_scale)\n",
    "style_images = [load_image(path, style_size) for path in style_paths]\n",
    "\n",
    "# set hyper-parameters\n",
    "stylenet.set_content_weight(5e0)\n",
    "stylenet.set_style_weight(1e3)\n",
    "stylenet.set_hist_weight(0)\n",
    "stylenet.set_tv_weight(1e-3)\n",
    "stylenet.set_style_statistic('gram')\n",
    "\n",
    "# something like this?\n",
    "#stylenet.set_style_layer(0, 'gram', 1e2)\n",
    "#stylenet.set_style_layer(1, 'covariance', 1e2)\n",
    "\n",
    "# capture the style and content images\n",
    "stylenet.capture(content_image, style_images)\n",
    "\n",
    "# initialize with a random image\n",
    "img = random_image_like(content_image)\n",
    "\n",
    "optimize(stylenet, img, num_iterations, output_path, original_colors, print_iter, save_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 800/800: \n",
      "  Style loss = 8.1e+02, 2.1e+04, 1.2e+04, 1.9e+05, 1.0e+02\n",
      "  TV loss = 1.2e+04\n",
      "  Total loss = 2.37e+05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[-43.0551, -42.8480, -33.1289,  ...,  -4.0490,  -4.2271,  -4.2849],\n",
       "          [-44.4197, -42.5732, -31.2063,  ...,  -4.2878,  -4.2909,  -4.3693],\n",
       "          [-44.7566, -43.3370, -21.0557,  ..., -14.9968, -12.7377, -12.7173],\n",
       "          ...,\n",
       "          [-68.6525, -68.6609, -63.4467,  ..., -32.4816,  13.4839,  39.9641],\n",
       "          [-27.9955, -27.9766, -18.9608,  ...,  39.3473,  39.9464,  39.9639],\n",
       "          [-27.9990, -27.9777, -18.9666,  ...,  39.3458,  39.9332,  39.9588]],\n",
       "\n",
       "         [[ 89.2776,  86.4382,  70.7958,  ...,  99.2659,  50.5392,  43.0732],\n",
       "          [ 89.2030,  78.4606,  66.3775,  ...,  55.5223,  50.5102,  -2.5931],\n",
       "          [ 74.6339,  64.5337,  53.3437,  ...,  -2.6036,  -2.6184,  -2.6347],\n",
       "          ...,\n",
       "          [-99.1302, -99.6419, -19.6715,  ..., -37.1758, -29.8861,  38.2951],\n",
       "          [-30.9289, -30.9133, -17.7217,  ...,  -0.7292,  35.6196,  38.3400],\n",
       "          [-30.9274, -30.9182, -17.7269,  ...,  -0.7243,  35.6173,  38.3418]],\n",
       "\n",
       "         [[ 70.0611,  68.6049,  38.7390,  ...,  51.9068,  32.2629,  32.1596],\n",
       "          [ 62.1391,  53.2309,  38.6078,  ...,  29.4118,  29.3503,  -4.5665],\n",
       "          [ 44.1708,  39.1859,  38.4085,  ..., -19.7636, -21.2464, -21.2995],\n",
       "          ...,\n",
       "          [-34.3311, -34.6069,  -7.5402,  ..., -33.2263, -33.0147, -21.7680],\n",
       "          [-35.0548, -35.0560,  -7.4179,  ..., -33.2270, -33.0167, -21.9566],\n",
       "          [-35.1071, -35.1061,  -7.6719,  ..., -33.2292, -33.0214, -32.3655]]]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "        \n",
    "image_size = 720\n",
    "num_iterations = 800\n",
    "output_path = 'out8.png'\n",
    "original_colors = 0\n",
    "style_scale = 1.0\n",
    "print_iter = 100\n",
    "save_iter = 100\n",
    "\n",
    "\n",
    "content_path = 'examples/inputs/hoovertowernight.jpg'\n",
    "style_paths = ['examples/inputs/starry_night.jpg','examples/inputs/cubist.jpg']\n",
    "content_mask_paths = ['examples/segments/hoovertowernight2a.png','examples/segments/hoovertowernight2b.png']\n",
    "\n",
    "# load content image\n",
    "content_image = load_image(content_path, image_size)#.type(dtype)\n",
    "\n",
    "# load style images\n",
    "style_size = int(image_size * style_scale)\n",
    "style_images = [load_image(path, style_size) \n",
    "                for path in style_paths]\n",
    "\n",
    "# load masks\n",
    "content_masks = [load_image(path, image_size, to_normalize=False) \n",
    "                 for path in content_mask_paths]\n",
    "\n",
    "# set hyper-parameters\n",
    "stylenet.set_content_weight(0)\n",
    "stylenet.set_style_weight(1e2)\n",
    "stylenet.set_hist_weight(0)\n",
    "stylenet.set_tv_weight(1e-3)\n",
    "stylenet.set_style_statistic('covariance')\n",
    "# error handling: if capture before, there are problems\n",
    "\n",
    "# capture the style and content images (with masks)\n",
    "stylenet.capture(content_image, style_images, None, content_masks)\n",
    "            \n",
    "\n",
    "# initialize with a random image\n",
    "img = random_image_like(content_image)\n",
    "\n",
    "optimize(stylenet, img, num_iterations, output_path, original_colors, print_iter, save_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1000/1000: \n",
      "  Content loss = 0.0e+00\n",
      "  Style loss = 2.4e+04, 3.1e+06, 1.2e+06, 4.0e+07, 3.0e+03\n",
      "  TV loss = 3.5e+04\n",
      "  Total loss = 4.39e+07\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[ 107.4170,  107.3822,  107.2753,  ...,   18.5964,   18.6580,\n",
       "             18.6587],\n",
       "          [ 107.4086,  107.3838,  107.2558,  ...,   18.5873,   18.6362,\n",
       "             18.6319],\n",
       "          [   6.0468,    6.0569,   26.0255,  ...,   18.5962,   18.6259,\n",
       "             18.6241],\n",
       "          ...,\n",
       "          [ -15.5395,  -15.5464,  -15.5937,  ...,   -4.7783,   -4.8542,\n",
       "             -4.8807],\n",
       "          [ -15.6088,  -15.5838,  -15.5704,  ...,   -4.8293,   -4.8587,\n",
       "             -4.8855],\n",
       "          [ -15.5850,  -15.5833,  -15.5725,  ...,   -4.8222,   -4.8541,\n",
       "             -4.8611]],\n",
       "\n",
       "         [[  20.6844,   20.6578,   20.5571,  ...,   20.3936,   20.4353,\n",
       "             20.4430],\n",
       "          [  20.6818,   20.6415,   20.5252,  ...,   20.3572,   20.4206,\n",
       "             20.4426],\n",
       "          [ -93.5266,  -94.0915,  -97.6999,  ...,   20.3577,   20.4144,\n",
       "             20.4342],\n",
       "          ...,\n",
       "          [ -40.6276,  -40.6149,  -40.6026,  ...,   13.2112,   13.2194,\n",
       "             13.2305],\n",
       "          [ -40.5604,  -40.5715,  -40.5721,  ...,   13.2204,   13.2089,\n",
       "             13.2494],\n",
       "          [ -40.5617,  -40.5551,  -40.5580,  ...,   13.2325,   13.2305,\n",
       "             13.1608]],\n",
       "\n",
       "         [[ -71.0882,  -71.1080,  -71.9495,  ...,   23.5993,   23.8394,\n",
       "             23.8973],\n",
       "          [ -71.1370,  -71.1350,  -71.9137,  ...,   23.5728,   23.8488,\n",
       "             23.8602],\n",
       "          [-116.0931, -113.4171, -113.0039,  ...,   23.5603,   23.8784,\n",
       "             23.9017],\n",
       "          ...,\n",
       "          [ -35.7993,  -35.8265,  -35.8149,  ...,   26.8732,   26.8550,\n",
       "             26.8414],\n",
       "          [ -35.7822,  -35.8037,  -35.8361,  ...,   26.8806,   26.8423,\n",
       "             26.8359],\n",
       "          [ -35.7844,  -35.7783,  -35.8108,  ...,   26.8794,   26.8500,\n",
       "             26.8926]]]], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#######################################################3\n",
    "\n",
    "\n",
    "image_size = 1280\n",
    "num_iterations = 1200\n",
    "output_path = 'go5.png'\n",
    "\n",
    "original_colors = 0\n",
    "style_scale = 0.25\n",
    "print_iter = 100\n",
    "save_iter = 100\n",
    "\n",
    "content_path = 'examples/inputs/hoovertowernight.jpg'\n",
    "\n",
    "\n",
    "style_paths = ['examples/inputs/starry_night.jpg','examples/inputs/hokusai.jpg',\n",
    "                'examples/inputs/starry_night.jpg','examples/inputs/hokusai.jpg',\n",
    "                'examples/inputs/starry_night.jpg','examples/inputs/hokusai.jpg',\n",
    "                'examples/inputs/starry_night.jpg','examples/inputs/hokusai.jpg',\n",
    "                'examples/inputs/starry_night.jpg','examples/inputs/hokusai.jpg',\n",
    "                'examples/inputs/starry_night.jpg','examples/inputs/hokusai.jpg']\n",
    "\n",
    "\n",
    "style_blend_weights = [1.0, 0.0,\n",
    "                      0.8, 0.2,\n",
    "                       0.6, 0.4,\n",
    "                       0.4, 0.6,\n",
    "                       0.2, 0.8,                      \n",
    "                      0.0, 1.0]\n",
    "\n",
    "# load content image\n",
    "content_image = load_image(content_path, image_size)\n",
    "\n",
    "# load style images\n",
    "style_size = int(image_size * style_scale)\n",
    "style_images = [load_image(path, style_size) for path in style_paths]\n",
    "\n",
    "stylenet.set_content_weight(0)\n",
    "stylenet.set_style_weight(1e2)\n",
    "stylenet.set_tv_weight(1e-3)\n",
    "\n",
    "# capture the style and content images\n",
    "\n",
    "# initialize with a random image\n",
    "img = random_image_like(content_image)\n",
    "\n",
    "\n",
    "\n",
    "content_masks = [torch.zeros(content_image.shape) for c in range(len(style_images))]\n",
    "m=8\n",
    "content_masks[0][:,:,:,:360+m] = 1.0\n",
    "content_masks[1][:,:,:,:360+m] = 1.0\n",
    "content_masks[2][:,:,:,360-m:500+m] = 1.0\n",
    "content_masks[3][:,:,:,360-m:500+m] = 1.0\n",
    "content_masks[4][:,:,:,500-m:640+m] = 1.0\n",
    "content_masks[5][:,:,:,500-m:640+m] = 1.0\n",
    "content_masks[6][:,:,:,640-m:780+m] = 1.0\n",
    "content_masks[7][:,:,:,640-m:780+m] = 1.0\n",
    "content_masks[8][:,:,:,780-m:920+m] = 1.0\n",
    "content_masks[9][:,:,:,780-m:920+m] = 1.0\n",
    "content_masks[10][:,:,:,920-m:] = 1.0\n",
    "content_masks[11][:,:,:,920-m:] = 1.0\n",
    "\n",
    "\n",
    "stylenet.capture(content_image, style_images, style_blend_weights, content_masks)\n",
    "\n",
    "optimize(stylenet, img, 1000, output_path, original_colors, print_iter, save_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (1280) must match the existing size (720) at non-singleton dimension 3.  Target sizes: [1, 3, 673, 1280].  Tensor sizes: [720]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-40ac66a054c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mcontent_masks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstyle_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mcontent_masks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mcontent_masks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m720\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mcontent_masks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mcontent_masks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m720\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (1280) must match the existing size (720) at non-singleton dimension 3.  Target sizes: [1, 3, 673, 1280].  Tensor sizes: [720]"
     ]
    }
   ],
   "source": [
    "import IPython.display\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "from io import BytesIO \n",
    "import IPython.display\n",
    "import numpy as np\n",
    "\n",
    "def showarray(a, fmt='png'):\n",
    "    a = np.uint8(np.array(a))\n",
    "    f = BytesIO()\n",
    "    PIL.Image.fromarray(a).save(f, fmt)\n",
    "    IPython.display.display(IPython.display.Image(data=f.getvalue()))\n",
    "\n",
    "x = 0\n",
    "content_masks = [torch.zeros(content_image.shape) for c in range(len(style_images))]\n",
    "content_masks[0][:,:,:,:x] = torch.linspace(0, 1, x)\n",
    "content_masks[0][:,:,:,x:] = torch.linspace(1, 0, 720-x)\n",
    "content_masks[1][:,:,:,:x] = torch.linspace(0, 1, x)\n",
    "content_masks[1][:,:,:,x:] = torch.linspace(1, 0, 720-x)\n",
    "\n",
    "showarray(np.transpose(np.array(255*content_masks[0][0]), [1,2,0]).astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#######################################################3\n",
    "\n",
    "\n",
    "image_size = 720\n",
    "num_iterations = 1200\n",
    "output_path = 'go6.png'\n",
    "\n",
    "original_colors = 0\n",
    "style_scale = 1.0\n",
    "print_iter = 100\n",
    "save_iter = 100\n",
    "\n",
    "content_path = 'examples/inputs/hoovertowernight.jpg'\n",
    "style_paths = ['examples/inputs/starry_night.jpg','examples/inputs/hokusai.jpg']\n",
    "\n",
    "# load content image\n",
    "content_image = load_image(content_path, image_size)\n",
    "\n",
    "# load style images\n",
    "style_size = int(image_size * style_scale)\n",
    "style_images = [load_image(path, style_size) for path in style_paths]\n",
    "\n",
    "stylenet.set_content_weight(0)\n",
    "stylenet.set_style_weight(1e2)\n",
    "stylenet.set_tv_weight(1e-3)\n",
    "\n",
    "# capture the style and content images\n",
    "\n",
    "# initialize with a random image\n",
    "img = random_image_like(content_image)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "content_masks = [torch.zeros(content_image.shape) for c in range(len(style_images))]\n",
    "content_masks[0][:,:,:,:] = torch.linspace(0, 1, 720)\n",
    "content_masks[1][:,:,:,:] = torch.linspace(1, 0, 720)\n",
    "#content_masks[2][:,:,:,480:] = 1.0\n",
    "\n",
    "def optimize2(stylenet, img, num_iterations, output_path, original_colors, print_iter=None, save_iter=None):\n",
    "    def iterate():\n",
    "        t[0] += 1\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        stylenet(img)\n",
    "        loss = stylenet.get_loss()\n",
    "        loss.backward()\n",
    "        \n",
    "        maybe_print(stylenet, t[0], print_iter, num_iterations, loss)\n",
    "        maybe_save(img, t[0], save_iter, num_iterations, original_colors, output_path)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    img = nn.Parameter(img.type(stylenet.dtype))\n",
    "    optimizer, loopVal = setup_optimizer(img, stylenet.params, num_iterations)\n",
    "    t = [0]\n",
    "    while t[0] <= loopVal:\n",
    "        optimizer.step(iterate)\n",
    "    \n",
    "    return img\n",
    "\n",
    "\n",
    "\n",
    "for z in range(8):\n",
    "    for x in range(0,720,3):\n",
    "        if z%2==0:\n",
    "            content_masks = [torch.zeros(content_image.shape) for c in range(len(style_images))]\n",
    "            content_masks[0][:,:,:,:x] = torch.linspace(0, 1, x)\n",
    "            content_masks[0][:,:,:,x:] = torch.linspace(1, 0, 720-x)\n",
    "            content_masks[1][:,:,:,:x] = torch.linspace(0, 1, x)\n",
    "            content_masks[1][:,:,:,x:] = torch.linspace(1, 0, 720-x)\n",
    "            bw = x/719.0;\n",
    "            style_blend_weights = [bw, 1.0-bw]\n",
    "        else:\n",
    "            content_masks = [torch.zeros(content_image.shape) for c in range(len(style_images))]\n",
    "            content_masks[0][:,:,:,:720-x] = torch.linspace(0, 1, x)\n",
    "            content_masks[0][:,:,:,720-x:] = torch.linspace(1, 0, 720-x)\n",
    "            content_masks[1][:,:,:,:720-x] = torch.linspace(0, 1, x)\n",
    "            content_masks[1][:,:,:,720-x:] = torch.linspace(1, 0, 720-x)\n",
    "            bw = (719.0-x)/719.0;\n",
    "            style_blend_weights = [bw, 1.0-bw]\n",
    "\n",
    "        stylenet.capture(content_image, style_images, None, content_masks)\n",
    "\n",
    "        img = optimize2(stylenet, img, 5, output_path, original_colors, print_iter, save_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def optimize(stylenet, img, num_iterations, output_path, original_colors, print_iter=None, save_iter=None):\n",
    "    def iterate():\n",
    "        t[0] += 1\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        stylenet(img)\n",
    "        loss = stylenet.get_loss()\n",
    "        loss.backward()\n",
    "        \n",
    "        maybe_print(stylenet, t[0], print_iter, num_iterations, loss)\n",
    "        maybe_save(img, t[0], save_iter, num_iterations, original_colors, output_path)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    img = nn.Parameter(img.type(stylenet.dtype))\n",
    "    optimizer, loopVal = setup_optimizer(img, stylenet.params, num_iterations)\n",
    "    t = [0]\n",
    "    while t[0] <= loopVal:\n",
    "        optimizer.step(iterate)\n",
    "    \n",
    "\n",
    "        \n",
    "image_size = 720\n",
    "num_iterations = 1000\n",
    "output_path = 'try9d.png'\n",
    "original_colors = 0\n",
    "style_scale = 1.0\n",
    "print_iter = 100\n",
    "save_iter = 100\n",
    "\n",
    "\n",
    "content_path = 'examples/inputs/hoovertowernight.jpg'\n",
    "style_paths = ['examples/inputs/starry_night.jpg', 'examples/inputs/hokusai1.jpg']\n",
    "\n",
    "# load content image\n",
    "content_image = load_image(content_path, image_size)#.type(dtype)\n",
    "\n",
    "# load style images\n",
    "style_size = int(image_size * style_scale)\n",
    "style_images = [load_image(path, style_size) \n",
    "                for path in style_paths]\n",
    "\n",
    "#style_images[1] = torch.cat([style_images[0], style_images[2]], axis=2)\n",
    "\n",
    "# load masks\n",
    "content_masks = [torch.zeros(content_image.shape) for c in range(len(style_images))]\n",
    "content_masks[0][:,:,:,:] = torch.linspace(0, 1, 720)\n",
    "content_masks[1][:,:,:,:] = torch.linspace(1, 0, 720)\n",
    "#content_masks[2][:,:,:,480:] = 1.0\n",
    "\n",
    "# capture the style and content images (with masks)\n",
    "stylenet.capture(content_image, style_images, None, content_masks)\n",
    "            \n",
    "# set hyper-parameters\n",
    "stylenet.set_content_weight(0)\n",
    "stylenet.set_style_weight(1e2)\n",
    "stylenet.set_tv_weight(1e-3)\n",
    "\n",
    "# initialize with a random image\n",
    "img = random_image_like(content_image)\n",
    "\n",
    "optimize(stylenet, img, num_iterations, output_path, original_colors, print_iter, save_iter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
